---
title: "PLSC30500, Fall 2022"
subtitle: "Week 7. Estimation (2)"
# author: "Andy Eggers & Molly Offer-Westort"
output: 
  xaringan::moon_reader:
    self_contained: true
    css: [default, uchicago_pol_meth.css]
    nature:
      highlightLines: true
      countIncrementalSlides: false
---


```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(fig.height=5, fig.width=8.5, fig.align="center", warning = F, message = F)
```



class: inverse, middle, center

# Conditional expectations


---

# Conditional expectation: definition

$$E [Y | X = x] = \sum_y y f_{Y|X}(y | x)$$ 

i.e. the expectation of $Y$ at some $x$. 

---

# Conditional expectation: illustration 

Suppose $f_X(x)$ is uniform, and $f_{Y|X}(y|x)$ normal, with mean of $3x + 8x^2 - 3x^3$ and variance of $9$.

Then what is $E [Y | X = x] = \sum_y y f_{Y|X}(y | x)$ at $x = 1$?

--

```{r, fig.height = 4.5, echo = F}
n <- 1000
dat <- tibble(x = runif(n, min = 0, max = 2.75)) |> 
  mutate(mu = 3*x + 8*x^2 - 3*x^3,
         y = rnorm(n, mean = mu, sd = 3))
p <- dat |> 
  ggplot(aes(x = x, y = y)) + 
  geom_point(alpha = .5, size = 2) +
  theme_bw() + 
  geom_line(aes(y = mu), col = "red", lwd = 2)
p
```



---

# Conditional variance

Two formulations: 

$$V[Y | X = x] = E[(Y - E[Y | X =x])^2 | X = x]$$
$$V[Y | X = x] = E[Y^2 | X = x] - E[Y | X =x]^2$$
--

```{r, fig.height = 5, echo = F}
p
```

---

# Conditional variance (2)


Two formulations: 

$$V[Y | X = x] = E[(Y - E[Y | X =x])^2 | X = x]$$

$$V[Y | X = x] = E[Y^2 | X = x] - E[Y | X =x]^2$$

```{r, echo = F}
tibble(x = runif(n, min = 0, max = 2.75)) |> 
  mutate(mu = 3*x + 8*x^2 - 3*x^3,
         y = rnorm(n, mean = mu, sd = 1 + x)) |> 
  ggplot(aes(x = x, y = y)) + 
  geom_point(alpha = .5, size = 2) +
  theme_bw() + 
  geom_line(aes(y = mu), col = "red", lwd = 2)
```


---

# CEF

Conditional expectation $E[Y | X = x]$ is for a specific $x$.

Conditional expectation function (CEF) $E[Y | X]$ is for all $x$.

--

Suppose again $f_X(x)$ is uniform, and $f_{Y|X}(y|x)$ normal, with mean of $3x + 8x^2 - 3x^3$ and variance of $9$.

Then what is $E [Y | X ]$?

```{r, echo = F, fig.height = 4.5}
p
```


---

# Law of iterated expectations

**Theorem 2.2.17**. *Law of iterated expectations*

For random variables $X$ and $Y$,

$$E[Y] = E[E[Y | X]]$$
--

In words: to get the average of $Y$, you can take the average of $Y$ within levels of $X$ and then average those according to the distribution of $X$.

---

# LIE: An intuitive example

A population is 80% female and 20% male.

The average age among females is 25. The average age among males is 20.

What is the average age in the population?

--

$$E[E[Y | X]] = .8 \times 25 + .2 \times 20 = 24$$

---

# LIE: another example

```{r, echo = F}
p
```


---

# LIE: another example (2)

```{r, echo = F}
n <- 5000
tibble(x = rnorm(n, mean = 3, sd = 1.5)) |> 
  mutate(mu = 3*x + 8*x^2 - 3*x^3,
         y = rnorm(n, mean = mu, sd = 3)) |> 
  filter(x > 0 & x < 2.75) |> 
  ggplot(aes(x = x, y = y)) + 
  geom_point(alpha = .5, size = 2) +
  theme_bw() + 
  geom_line(aes(y = mu), col = "red", lwd = 2)
```

---

## How LIE is used (preview)

Suppose we want to estimate

$$\text{ATE} = E[Y_i(1) - Y_i(0)] = E[Y_i(1)] - E[Y_i(0)]$$.

--

How did we estimate $E[Y_i(1)]$ and $E[Y_i(0)]$ via randomization?

--

Suppose randomization impossible. By LIE,
$$\begin{aligned} 
E[Y_i(1)] &= E[E[Y_i(1) | X_i]] \\\
 &= \sum_x E[Y_i(1) | X_i = x] \text{Pr}[X_i = x]
\end{aligned}$$
(And same for $Y_i(0)$.)

--

Now suppose $E[Y_i(1) | X_i = x] = E[Y_i | D_i = 1, X_i = x] \, \forall x$ (**strong ignorability**), and same for $Y_i(0)$.

--

Then we can plug in above, and the ATE is **identified**.

???

Intuitively, is there a covariate such that, conditional on that covariate, the observed outcomes for the treated units are the counterfactual values for the control units with those same values of the covariate?

If so, by LIE (and SUTVA), you have identified the treatment effect.

---

# Law of total variance

$$V[Y] = E[V[Y|X]] + V[E[Y|X]]$$
--

```{r, echo = F}
p
```

---

# Law of total variance (2)

Let's take the data points in `dat` as the population: 

```{r}
head(dat, 4)
```

Then: 

```{r}
dat |> 
  summarize(var(y), var(y - mu), var(mu))
```


---

# CEF as best predictor

The CEF $E[Y | X]$ is the expectation of $Y$ at each $X$.

--

We already (wk. 3, chap. 2) established that the expectation/mean is the best (in MSE sense) predictor.

--

So CEF is the best possible way to use $X$ to predict $Y$. 

---

# Best *linear* predictor (BLP)

Say we focus on a linear predictor, i.e. a function of the form $\alpha + \beta X$.

--

The best (minimum MSE) predictor satisfies

$$(\alpha, \beta) = \underset{(a,b) \in \mathbb{R}^2}{\arg\min} \, \mathrm{E}\,[\left(Y - (a + bX)\right)^2]$$
--

The solution (see Theorem 2.2.21) is 

- $\beta = \frac{\textrm{Cov}[X, Y]}{\textrm{V}[X]}$
- $\alpha = \textrm{E}[Y] - \beta \textrm{E}[X]$ 

--

So we could obtain the BLP from a joint PMF. (See homework.)

---

# BLP predicts CEF

Above, we were looking for best linear predictor (BLP) of $Y$ as function of $X$: 

$$(\alpha, \beta) = \underset{(a,b) \in \mathbb{R}^2}{\arg\min} \, \mathrm{E}\,[\left(Y - (a + bX)\right)^2]$$

--

You get the same answer if you look for the best linear predictor of the CEF $E[Y | X]$, i.e.

$$(\alpha, \beta) = \underset{(a,b) \in \mathbb{R}^2}{\arg\min} \, \mathrm{E}\,[\left(\mathrm{E}[Y|X] - (a + bX)\right)^2]$$

```{r, echo = F, fig.height = 3.5}
p + 
  geom_smooth(method = lm)
```




