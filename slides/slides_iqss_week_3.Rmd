---
title: "PLSC30500, Fall 2022"
subtitle: "Week 3. Summarizing distributions"
# author: "Andy Eggers & Molly Offer-Westort"
output: 
  xaringan::moon_reader:
    self_contained: true
    css: [default, uchicago_pol_meth.css]
    nature:
      highlightLines: true
      countIncrementalSlides: false

---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(fig.height=5, fig.width=8.5, fig.align="center", warning = F, message = F)
```


<!-- TODO based on 2022 version: add something comparing MSE to mean deviation and MAD --> 

# Big picture/motivation

For now, we are talking about probability and random variables.

--

Later (and previously), we worked with datasets.

--

What is the connection?

--

<br>

**Probability**: Start with random variable, make statements about what data might be observed.

**Statistics**: Start with data, make statements about what random variable might have produced it.

--

<br>

What is random about the process producing the data? 

- random sampling
- random assignment (into treatment)
- complex social phenomena

---

## Population and sample (preview)

The properties that characterize RVs are not observable in (finite) data:

- distribution function (PDF, PMF, joint distribution)
- expected value/mean
- variance, covariance
- conditional expectation function (CEF)

--

Sometimes we say these relate to the *population* (*population parameters*).

--

We collect *samples* and describe them in terms of the *sample analogues* of the above properties: 

- frequency tables/plots
- sample mean
- sample variance, sample covariance
- regression

--

But remember that our target (*estimand*) is the *population parameter*, and our sample statistics are random variables.  


---


class: inverse, middle, center

# Summarizing the "location" of a random variable


---


# Expected value: discrete case 

For discrete R.V. with probability mass function (PMF) $f$, the *expected value* of $X$ is 

$$E[X] = \sum_x x f(x) $$

Could write 

$$E[X] = \sum_{x \in \text{Supp}[X]} x f(x) $$

---

# Example 

.pull-left[
| $x$  | $P(X = x)$ |
|------|:------------:|
|   0   |     .2     |
|   1   |     .5     |
|   3   |     .3     |
]

.pull-right[
$$
f(x) = \begin{cases}
.2 & x = 0 \\\
.5 & x = 1 \\\
.3 & x = 3 \\\
0 & \text{otherwise}
\end{cases}
$$
]

<br>


What is $E[X] = \sum_x x f(x)$?

--

$$\begin{aligned}
E[X] &= \sum_x x f(x) \\
 &= 0 \times .2 + 1 \times .5 + 3 \times .3 \\
 &= 1.4
\end{aligned}$$


---

# Same PMF

```{r, echo = F}
dat <- tibble(x = c(0, 0, 1, 1, 3, 3), y = c(0, .2, 0, .5, 0, .3), item = c(1, 1, 2, 2, 3, 3), type = rep(c("bottom", "top"), 3)) 

dat|> 
  ggplot(aes(x = x, y = y, group = item)) + 
  geom_line() + 
  geom_point(data = filter(dat, type == "top")) + 
  expand_limits(y = .65) +
  geom_vline(xintercept = .5*1 + .3*3, lty = 2)
```

---

# E[X] as X's center of mass 

Suppose we place a weight $f(x)$ at each value $x \in \text{Supp}(X)$ along a weightless rod.

--

Where is the **center of mass**, i.e. the point where the rod balances?

--

It is the point $c$ where $\sum_x (x - c) f(x) = 0$.

--

That point is $E[X]$.

--

**Proof**:

$$\begin{aligned} \sum_x (x - E[X]) f(x) &= \sum_x \left( x f(x) - E[X] f(x) \right) \\
&= \sum_x x f(x) -  \sum_x E[X] f(x) \\
 &= E[X] - E[X] \sum_x f(x) \\
 &= E[X] - E[X] \\
 &= 0
\end{aligned}$$

---


# Expectation vs average

Given a set of numbers $x_1, x_2, \ldots, x_n$ we can take the **average**: 

$$ \overline{x} = \frac{1}{n} \sum_{i = 1}^n x_i $$

What is the connection between $E[X] = \sum_x x f(x)$ and $\overline{x} = \frac{1}{n} \sum_{i = 1}^n x_i$?

--

- $E[X]$ summarizes a random variable $X$; $\overline{x}$ summarizes a set of numbers
- if each $x_i$ is an (iid) **sample** from $X$, $\overline{x}$ approximates $E[X]$
- if each $x$ appears with frequency $f(x)$, then $\overline{x} = E[X]$

---

# Expectation vs average (2)

Given PMF:

$$
f(x) = \begin{cases}
.2 & x = 0 \\\
.5 & x = 1 \\\
.3 & x = 3 \\\
0 & \text{otherwise}
\end{cases}
$$

Then $E[X] = 0 \times .2 + 1 \times .5 + 3 \times .3 = 1.4$.

--

Alternative method: make a vector of length $n$ where each $x$ appears $n f(x)$ times: 

```{r}
x <- c(0, 0, 1, 1, 1, 1, 1, 3, 3, 3)
mean(x)
```

--

Why does this work? 

---

# Expectation vs average (3) 

If each unique $x$ appears $n f(x)$ times, then

$$\begin{aligned} \overbrace{\frac{1}{n} \sum_i x_i}^{\text{Average}} &= \frac{1}{n} \sum_x x   n f(x)  \\
&= \sum_x x  f(x) = E[X] \end{aligned}$$

--

May clarify why, in any given sample, $\overline{x} \neq E[X]$.


---

# The continuous case 

For continuous R.V. $X$, 

$$ E[X] = \int_{-\infty}^{\infty} x f(x) dx $$ 

--

```{r, echo = F}
xs <- seq(-.5, 2, by =.01)
ys <- dnorm(xs)

# get expectation numerically -- two options

# weighted sum of xs in my sequence
ex1 <- sum(xs*ys)/sum(ys)

# average of a sample
x_samp0 <- rnorm(100000)
x_samp <- x_samp0[x_samp0 > -.5 & x_samp0 < 2]
ex2 <- mean(x_samp)

# plot it
tibble(x = c(-.5, xs, 2), y = c(0, ys, 0)) |> 
  ggplot(aes(x = x, y = y)) + 
  geom_polygon(fill = "darkgray") + 
  theme_bw() + 
  geom_vline(xintercept = ex1, col = "red") # +
  # geom_vline(xintercept = ex2, col = "blue") # 

```

???

Just convey idea that balance will still work, we won't be doing integration.

And we will stick to the discrete case as much as possible.

---

# Expectation of a function of an R.V. (Theorem 2.1.5)

Let $g(X)$ be a function of discrete random variable $X$. It is a random variable with PMF $f(g(X))$. 

(e.g. $g(X) = 3X$)


Then $$ E[g(X)] = \sum_{x \in \text{Supp}(X)} g(x) f(x)$$

<!-- $$\begin{aligned} -->
<!-- E[g(X)] &= \sum_{x \in \text{Supp}(g(X))} g(X) f(g(x)) \\ -->
<!-- &= \sum_{x \in \text{Supp}(X)} g(x) f(x) \\ -->
<!-- \end{aligned}$$. -->


--


Why is this the same as $\sum_{x \in \text{Supp}(g(X))} g(x) f(g(x))$?

| $x$  | $P(X = x)$ | $g(x)$ | $P(g(X) = g(x))$
|------|:------------:|------|:-------:|  
|   0   |     1/3     |  0 |  1/3 |
|   1   |     2/3    |  3 |  2/3 |



---

# E[X] as a "good guess"

Define **mean squared error** of a R.V. $X$ about $c$:

$$ E[(X - c)^2] $$

--

For $c=1$, we have:

| $x$  | $f(x)$ | $(x - 1)^2$ | 
|------|:------------:|:------:|  
|   0   |   .2     |  1 |  
|   1   |     .5    |  0 |  
| 3 |   .3  |  4 | 

--

So MSE of $X$ about $1$ is: 

$$
.2 \times 1 + .5 \times 0 + .3 \times 4 = 2.2 
$$


--

$E[X]$ is the choice of $c$ that minimizes MSE. (Theorem 2.1.24, illustrated on homework.)


---

# Expectation of a function two RVs

Consider this joint PMF $f(x, y)$ for $X$ and $Y$

.pull-left[
| $x$ | $y$ | $f(x,y)$
|-----|----|:------------:|  
|   0   | 0 |     1/10     |  
|   0   | 1 |    1/5 | 
|  1 | 0 |    1/5 | 
| 1 |  1 |   1/2 |
]

.pull-right[
$$
f(x,y) = \begin{cases}
1/10 & x = 0, y = 0 \\\
1/5 & x = 0, y = 1 \\\
1/5 & x = 1, y = 0 \\\
1/2 & x = 1, y = 1 \\\
0 & \text{otherwise}
\end{cases}
$$

]

--

What is $E[XY]$? 

--

$$E[XY] = \sum_x \sum_y xy f(x, y)$$
--

$$\begin{aligned}
E[XY] &= \sum_x \sum_y xy f(x, y) \\
&= 0 \times 1/10 + 0 \times 1/5 + 0 \times 1/5 + 1 \times 1/2 \\
 &= 1/2 
\end{aligned}$$


---

class: inverse, middle, center

# Summarizing the "spread" of a random variable


---


# Variance 

$$V[X] = E[(X - E[X])^2]$$
--

An example: 

$$
f(x) = \begin{cases}
1- p & x = 0 \\\
p & x = 1 \\\
0 & \text{otherwise}
\end{cases}
$$

--

What is $E[X]$?

--

| $x$ | $f(x)$ | $(x - E[X])^2$|
|-----|:----:|:---:|  
|   0   | $1-p$ | $p^2$ |
|   1   | $p$ |  $1 - 2p + p^2$ |

$$\begin{aligned}
V[X] &= E[(x - E[X])^2] \\ &= p^2(1-p) + (1 - 2p + p^2)p \\ &= p^2 - p^3 + p - 2p^2 + p^3 \\ &= p(1 - p)\end{aligned}$$


---

# Variance: alternative formulation


Same example: 

$$
f(x) = \begin{cases}
1- p & x = 0 \\\
p & x = 1 \\\
0 & \text{otherwise}
\end{cases}
$$

--

Alternative formulation for variance: 

$$V[X] = E[X^2] - E[X]^2$$

--

What is $E[X]$? What is $E[X^2]$?   

--

By this alternative formula, we then have 

$$V[X] = p - p^2 = p(1-p)$$

---

<!-- # Chebysev's Inequality -->

<!-- For all $\epsilon > 0$, and with $\sigma[X]$ denoting the standard deviation of $X$ (i.e. $\sqrt{V[X]}$), -->

<!-- $$\text{Pr}\left[ |X - E[X]| \geq \epsilon \sigma[X] \right] \leq \frac{1}{\epsilon^2}$$ -->
<!-- What does this mean?  -->

<!-- I want to understand this but I don't think it's that important for the students. -->

<!-- Looking at the binding case in Wikipedia would be instructive. Maybe also looking at symmetrical about 0 pmf with just two points. -->

<!-- -- -->

<!-- Consider an example PMF, with assumption $p < .5$. -->

<!-- | $x$ | $f(x)$ | -->
<!-- |-----|:----:|   -->
<!-- |   0   | $1-p$ | -->
<!-- |   1   | $p$ |  -->

<!-- -- -->

<!-- We know $\sigma[X] = \sqrt{p(1-p)}$.  -->

<!-- So Chebychev's inequality tells us that for all $\epsilon$ and $p \in (0,1)$, -->

<!-- $$\text{Pr}\left[ 1 - p \geq \epsilon p(1-p) \right] \leq \frac{1}{\epsilon^2}$$ -->



# Parametric distributions

Given any RV $X$ with associated PMF/PDF or CDF, we can compute (in principle)

- $E[X]$
- $V[X]$
- etc

--

For special types of RV, these summaries are the parameters that define the distribution:

- Bernoulli distribution identified just by $E[X]$ (i.e. $p$)
- Normal distribution identified just by $E[X]$, $V[X]$ (i.e. $\mu$ and $\sigma^2$)

--

But don't get confused: any RV has $E[X]$ and $V[X]$, not just these special ones.

---

class: inverse, middle, center

# Summarizing relationships (joint distributions)


---

# Covariance 

$$\text{Cov}[X, Y] = E\left[  (X - E[X])(Y - E[Y]) \right]$$ 

--

Intuitively, "Does $X$ tend to be above $E[X]$ when $Y$ is above $E[Y]$? (And by how much?)"

--

$$
f(x,y) = \begin{cases}
1/10 & x = 0, y = 0 \\\
1/5 & x = 0, y = 1 \\\
1/5 & x = 1, y = 0 \\\
1/2 & x = 1, y = 1 \\\
0 & \text{otherwise}
\end{cases}
$$

What is $E[X]$? What is $E[Y]$? 

--

Then compute expectation of $(X - E[X])(Y - E[Y])$ (function of two RVs) as above.

---

# Geometric interpretation

For each point $x,y$, construct a cube with width $x - E[X]$, height $y - E[Y],$ and depth $f(x,y)$. Add the "volumes".

$$
f(x,y) = \begin{cases}
1/10 & x = 0, y = 0 \\\
1/5 & x = 0, y = 1 \\\
1/5 & x = 1, y = 0 \\\
1/2 & x = 1, y = 1 \\\
0 & \text{otherwise}
\end{cases}
$$

```{r, echo = F, fig.height=4}
ex <- 6/10
ey <- 6/10
bind_rows(
  tibble(x = c(0, 0, ex, ex), y = c(ey, 0, 0, ey), lab = rep("A", 4), f = 1/10),
  tibble(x = c(0, 0, ex, ex), y = c(ey, 1, 1, ey), lab = rep("B", 4), f = 1/5),
  tibble(x = c(1, 1, ex, ex), y = c(ey, 0, 0, ey), lab = rep("C", 4), f = 1/5),
  tibble(x = c(1, 1, ex, ex), y = c(ey, 1, 1, ey), lab = rep("D", 4), f = 1/2)
  ) -> dat

dat |> 
  ggplot(aes(x = x, y = y, group = lab)) +
  geom_polygon(aes(fill = f)) + # , col = "black") + 
  geom_vline(xintercept = ex, lty = 2) + 
  geom_hline(yintercept = ey, lty = 2) +
  geom_point(data = tibble(x = c(0, 0, 1, 1), y = c(0, 1, 0, 1), lab = LETTERS[1:4]), size = 3) + 
  labs(alpha = "f(x, y)") + 
  coord_fixed() -> gg 

gg
```


---

## Alternative formulation

First formulation: 

$$\text{Cov}[X, Y] = E\left[  (X - E[X])(Y - E[Y]) \right]$$ 

--

As with variance, an alternative formulation: 

$$\text{Cov}[X, Y] = E\left[XY\right] - E[X]E[Y]$$ 

--

Note: 

- if $E[X] = E[Y] = 0$ (e.g. if recentered), the two formulations look identical
- geometrically, can think in terms of areas of rectangles


---

# Linearity of expectations, but not variances 

If $f$ is  a *linear function* or *linear operator*, then $f(x + y) = f(x) + f(y)$. (**Additivity** property.)

--

<br>

$$E[X + Y] = E[X] + E[Y]$$ 

Why? 

--
<br>



$$\text{Var}[X + Y] \neq \text{Var}[X] + \text{Var}[Y]$$ 

Why not? 

<br>


---

# Variance rule (non-linearity of variance)
### A different proof than A&R 2.2.3

$$\begin{aligned}
\text{Var}(X+Y) &= E[(X + Y - E[X + Y])^2] \\\
&= E[(X - E[X] + Y - E[Y])^2] \\\
&= E[(\tilde{X} + \tilde{Y})^2] \\\
&= E[\tilde{X}^2 + \tilde{Y}^2 + 2 \tilde{X} \tilde{Y}] \\\
&= E[\tilde{X}^2] + E[\tilde{Y}^2] + E[2 \tilde{X} \tilde{Y}] \\\
&= E[((X - E[X])^2] + E[(Y - E[Y])^2] + 2E[(X - E[X])(Y - E[Y])] \\\
&= \text{Var}(X) + \text{Var}(Y) + 2\text{Cov}(X, Y)
\end{aligned}$$


---

# Back to the bigger picture

We have talked about properties of random variables.

These properties depend on the PMF/PDF or CDF (possibly joint).

--

<br>

In data problems, the RV's PMF/PDF is not known; we just have a sample.

The goal is to infer properties of the RV from features of the sample.

--

Don't forget that as you work with data!  



<!-- --- -->


<!-- # Variance of a function of two RVs -->

<!-- What is $\text{Var}(X + Y)$?  -->

<!-- $$ -->
<!-- f(x,y) = \begin{cases} -->
<!-- 1/10 & x = 0, y = 0 \\\ -->
<!-- 1/5 & x = 0, y = 1 \\\ -->
<!-- 1/5 & x = 1, y = 0 \\\ -->
<!-- 1/2 & x = 1, y = 1 \\\ -->
<!-- 0 & \text{otherwise} -->
<!-- \end{cases} -->
<!-- $$ -->

<!-- -- -->

<!-- Recall $\text{Var}(X) = E[(X - E[X])^2]$. -->

<!-- Letting $\tilde{X} \equiv X + Y$, we can simplify as   -->

<!-- $$ -->
<!-- f(\tilde{x}) = \begin{cases} -->
<!-- 1/10 & \tilde{x} = 0 \\\ -->
<!-- 2/5 & \tilde{x} = 1 \\\ -->
<!-- 1/2 & \tilde{x} = 2 \\\ -->
<!-- 0 & \text{otherwise} -->
<!-- \end{cases} -->
<!-- $$ -->
<!-- -- -->

<!-- $E[\tilde{X}] = .4 \times 1 + .5 \times 2 = 1.4$, so  $E[(\tilde{X} - E[X])^2] = .1 \times 1.4^2 + .4 \times .4^2 + .5 \times .6^2 = .44$ -->





